#!/usr/bin/env python
import glob
import logging
import os
import sys
import shutil
from argparse import ArgumentParser
from timeit import default_timer as timer

from ppdb_nba import ppdb_NBA
# Setup logging

logger = None

def setup_logging():
    loginstance = logging.getLogger('ppdb_nba')
    formatter = logging.Formatter(u'%(asctime)s - %(levelname)s - %(message)s')

    fh = logging.FileHandler(filename='ppdb_nba.log')
    fh.setLevel(logging.DEBUG)
    fh.setFormatter(formatter)

    sh = logging.StreamHandler()
    sh.setLevel(logging.DEBUG)
    sh.setFormatter(formatter)

    loginstance.addHandler(sh)
    loginstance.addHandler(fh)

    return loginstance


def scan_jobs(pp, args):
    """
    Scan the jobs directory specified in the config.yml in paths.jobs

    :param pp:
    :param args:
    :return:
    """
    if pp.is_locked():
        logger.info('Lockfile found, ppdb_nba still busy importing?')
        return False

    jobspath = pp.config.get('paths').get('jobs')

    # list all the job files, order by mtime
    jobs = glob.glob(jobspath + '/*.json')
    jobs.sort(key=os.path.getmtime)
    jobs.reverse()

    if len(jobs) > 0:
        # if there are jobs, pick the first one
        job = jobs.pop()
        jobfile = job.split('/')[-1]
        logger.info("{job} started".format(job=job))

        # handle the job (json) file
        if pp.handle_job(job, args.current):
            logger.info("SUCCESS: {job} completed".format(job=job))
            shutil.move(
                job,
                os.path.join(pp.config.get('paths').get('processed'),jobfile)
            )
        else:
            logger.error("{job} failed".format(job=job))
            shutil.move(
                job,
                os.path.join(pp.config.get('paths').get('failed'),jobfile)
            )
    else:
        logger.info('No jobs - nothing to do')
        return False


def import_incremental(pp, args):
    file = args.files[0]

    pp.set_source(args.source)
    if not args.force and not pp.lock_datafile(file):
        msg = "Lockfile found. Import of '{file}' still busy " \
              "importing?".format(file=file)
        logger.fatal(msg)
        print('To force an input: --force')
        exit(2)

    start = timer()
    logger.info("START incremental importing of {source}: {file}".format(source=args.source, file=file))
    try:
        pp.import_data(table=pp.sourceConfig.get('table') + '_import', datafile=file)
        pp.unlock_datafile(file)
    except Exception:
        exit(2)

    pp.remove_doubles()
    pp.handle_changes()
    logger.info(
        "[{elapsed:.2f} seconds] END incremental importing of {source}: {file}".format(
            elapsed=(timer() - start),
            source=args.source,
            file=file))


def import_to_current(pp, args):
    pp.set_source(args.source)
    logger.info("Fill current table '%s' with data" % (pp.sourceConfig.get('table') + '_current'))
    pp.import_data(pp.sourceConfig.get('table') + '_current', args.files[0])
    pp.remove_doubles(suffix='current')
    pp.set_indexes(pp.sourceConfig.get('table') + '_current')


def import_deleted(pp, args):
    pp.set_source(args.source)
    logger.info("Import deleted ids for source %s" % (pp.sourceConfig.get('table')))
    pp.import_deleted(args.files[0])


def export_source(pp, args):
    if len(args.files) > 0:
        exportfile = args.files[0]
        try:
            with open(exportfile,"w") as fp:
                pp.export_records(fp)
        except Exception:
            msg = '"{filename}" cannot be written'.format(filename=exportfile)
            logger.fatal(msg)
            sys.exit(msg)
    else:
        pp.set_source(args.export)
        pp.export_records()


def main():
    global logger
    logger = setup_logging()

    parser = ArgumentParser(usage='ppdb_nba --source sourcename /path/file1'
                                  '\n\nPreprocessing data to create incremental updates')

    parser.add_argument('--source',
                        action='store',
                        help='Name of the data source')
    parser.add_argument('--export',
                        action='store',
                        help='Name of the source that should be exported')
    parser.add_argument('--truncate',
                        action='store_true',
                        help='Truncate import and current tables of source before importing')
    parser.add_argument('--config',
                        action='store',
                        help='Config file',
                        default=False)
    parser.add_argument('--current',
                        action='store_true',
                        help='Import data directly to current table (default is normal incremental import)')
    parser.add_argument('--delete',
                        action='store_true',
                        help='Handle permanent deletes (default is normal incremental import)')
    parser.add_argument('--force',
                        action='store_true',
                        help='Ignore lockfiles, to force the import')
    parser.add_argument('--createtables',
                        action='store_true',
                        help='Generate database tables needed for importing')
    parser.add_argument('--debug',
                        action='store_true',
                        help='Set debugging level logging')
    parser.add_argument('--nologging',
                        action='store_true',
                        help='Do not log to the elastic search log')
    parser.add_argument('files',
                        help='One json data file',
                        nargs='*',
                        default=[])

    args = parser.parse_args()

    if (args.debug):
        logger.setLevel(logging.DEBUG)
    else:
        logger.setLevel(logging.INFO)

    configfile = args.config
    if not configfile:
        configfile = os.environ.get('PERCOLATOR_CONFIG', 'config.yml')

    pp = ppdb_NBA(config=configfile)
    pp.generate_mapping(create_tables=args.createtables)
    if args.nologging:
        pp.set_nologging()

    if not pp.config:
        logger.fatal("Configuration file '{config}' missing".format(config=configfile))
        exit(1)

    if args.source:
        pp.set_source(source=args.source)

    if args.truncate:
        pp.clear_data(table=pp.sourceConfig.get('table') + '_current')
        pp.clear_data(table=pp.sourceConfig.get('table') + '_import')

    if len(args.files) > 0:
        if args.current:
            import_to_current(pp, args)
            # @todo: import current in sources die enriched moeten worden, moeten op een of
            # andere manier toch nog enriched worden
        elif args.delete:
            import_deleted(pp, args)
        elif len(args.files) > 0:
            import_incremental(pp, args)
    else:
        if args.export:
            export_source(pp, args)
        else:
            scan_jobs(pp, args)


if __name__ == "__main__":
    main()
